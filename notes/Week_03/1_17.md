**Notes for Tuesday, January 7: Asymptotic Complexity**

# Introduction 
1. This is the last introductory lecture for DSA 
  1. We are going to be sorting out what "cheap" and "expensive" 
  mean in computer science 
2. Self-Evaluation 
  1. Goal
    1. Evaluate your code beyond functional correctness
    2. Other things to consider 
  2. Look at what you need to pay attention to 

3. Self-Quizzes
  1. Two Quizzes
    1. Asymptotic Complexity 
    2. Graphs 
      1. Look at Chapter 10 in the textbook 
      2. It is assumed that we have read that chapter by the lecture on 1/26 (next Thursday) 
  2. Instant feedback + Unlimited retries 
  3. Grade modifier 

# What is Asymptotic Complexity? 
1. Comparing different types of functions 
  1. Note how for different solutions in code, we will use different techniques
    1. Using loops vs not using loops 
      1. Loops is dependent on the size of the list 
      2. Whereas no loops is more efficient, since we can just compute instantaneously 

2. Breaking down a traverser example 
  ```
  def list_nth(list, n):
    let link = lst.head
    for i in range(n):
      link = link.next
    return link.data
  ```
  1. T(n) = time to do thing on an input size of size 'n' (number of elements) 
  2. Where T(n) = list_nth 
    T(n) = T(get.head) + T(setup for loop) + n * T(get.next) + n * T(assign link) + n * T(next for) + T(get data)  
    c1 = Tget.head + T(setup for) + T(get.data) 
    c2 = T(get.next) + T()
3. Now we can compare the linked list vs the array 
  1. for finding the nth element, it requires us to calculate c1 + c2*n for the list, while it is constant for the array 
  2. for pushing elements to the front, it requires a constant time for linked lists, but it requires us to calculate 
  f1 + f2*n
    1. Note, that no matter the amount of constant time it takes, say for an array to locate an element, it will ALWAYS be 
    slower than doing the calculation

4. Complexity Classes 
  1. c1 + c2n and f1 +f2n are similar expressions
    1. We can see that they are dependent on the variable 'n' and its value. They are *proportional*
      1. Linear functions
      2. Represented in Big-O Notation as ( O(n) )  
  2. d1  and e1 are also similar expressions in this sense 
    1. They are known as constant functions 
    2. Constant time is represented in Big-O Notation as ( O(1) ) 
  3. 
  4.
  5
5. Time Complexity Example 
  1. Searching a sorted array 
    1. Essentially, we just want to see within the sorted array and make sure
    that the element 
      1. Naive Method
        1. Just cycling through the list and comparing each element
        to the target number
      2. Binary Search
  2. Linear Search code 
  ```
  def linear_search (numbers, target): 
    for x in numbers: 
      if x == target: 
        return True 
      return False 
  ```
    1. Time Complexity Breakdown 
      1. Tlinear_search(n) = Tsetup for + n * (T== + T next for) 
      2. Tlinear_search(n) = c1 + c2n 
    
  3. Binary Search Code 
  ```
  def binary_search(numbers, target): 
    def helper (low, high): 
      if low > high:  return False 
      # empty range -> not found 
      let mid = ( low + high ) // 2 
      if numbers[mid] == target: return True 
      elif numbers[mid] < target:
        return helper(mid+1, high) 
      else: # numbers[mid] -> target 
        return helper(low, mid-1) # <- recursion 
    return helper(0, numbers.len()-1) 
  ```
  1. We rely on sortedness to cut out half the numbers each step 
  2. Key idea: 
    1. We want to do binary search on an array that is half the size 
    1. Time Complexity Breakdown 
  3. Time Complexity Breakdown 
    1. Tbinary_search(n) = T> + Tmid + Tarray_access + T== + T< + Tadd/sub + Tbinary_search(n/2)
    2. Tbinary_search(n) = c1 + Tbinary_search(n/2)
    3. So what is the time complexity of Tbinary_search(n)? 
      1. Tbinary_search(n) = c1 + Tbinary-search(n/2)
      2. c1 + c1 + Tbinary_search(n/4) 
      3. c1 + c1 + c1 + Tbinary_search(n/8) 
        1. And so on and so forth... 
      4. Logarithmic time complexity 
        1. Notated as ( O(log2n) ) <- big-O of log base 2 of n 

  4. What is the worst possible case? 
    Tbinary_search(1) = T> # <- we are left with a single element and we must return False 

6. So Linear Search or Binary Search? 
  1. Linear search takes O(n) while Binary Search takes O(log2n) 
  2. How does this translate to array size? 
  3. n, log2n
    16, 4 or log2(16)
    32, 5 or log2(32)
    64, 6 or log2(64) 
    65536, 16 or log2(65536)
  4. But what if we have some constant that makes O(log2n) so much more costly? 
    1. Linear time will catch up, no matter what, even if it took 1,000,000 more time for log2n to compute 
    2. Based on the nature of logarithms, the graph of O( log2n ) will eventually plateau, whereas the linear 
    search will continue to grow 

# The Formal / Mathematical Explanation (at least, the pre-CS_212 explanation) 
1. If 'f' is a function, then O(f) is the set of functions that "grow no faster than" 'f'
  1. "'g' grows no faster than 'f'" means that there exists some constants 'c' and 'm' such that for all
  n > m, g(n) <= c * f(n) 
    1. Essentially, as n approaches some arbitrarily large number (ex. infinity), the function g(n) will be 

  2. Intuitively, on large enough input (m), g grows no faster than f up to a change of constants (c) 

  3. O(n) implicitly refers to f(n) = n 

  4. This implies that f(n) is a set of all O(log2n) it is also a subset of all O(n). Therefore, binary search is
  TECHNICALLY a time complexity of O(n), but more accurately, we want to say O(log2n) 

  5. f <<< g means f is a subset of O(g) but g is not a subset of O(f) 
    1. For example, log2n <<< n, since log2n is a subset of O(n); however, n is not a subset of O(log2n)

# Big-O Equalities
1. Now we're just going to simplify what we have been doing a fit 
  1. O(f(n) + c) = O(f(n)) 
  Equivalently: f(n) + c is a subset of O(f(n)) 
  In other words: adding constants is negligible since f(n) + c is just a slightly more complexly 
    1. If you buy a car that comes with a hotdog, you still just bought a car, that's the most important part 

  2. O(c*f(n)) = O(f(n)) 
    1. Multiplying constants doesn't matter
      1. This goes back to the previous condition of g(n) <= c * f(n)

  3. O(logkf(n)) = O(logj(f(n))
    1. Log bases don't matter 
    2. Changing bases is all reliant on just shifting the amount of constants / whichever constants are multiplied
      1. Using constants is negligible
      2. Therefore, log base is irrelevant 

  4. O(f(n) + g(n)) = O(f(n)) if g <<< f 
    1. Adding smaller things doesn't matter, which is derived from the first rule, that we only really care about the dominant

  5. There also exists quadratic and cubic time complexities 
    1. O(f(n) * g(n)) = O(n^2) 

# Sorting Example  
1. Classic problems in computer science 
2. Given a list of numbers, return a list with the same elements, sorted in increasing order 
  1. There are many algorithms to solve this problem 
    1. Selection Sort 
      1. The process
        1. Find the largest element of the list 
        2. Remove the element from the input list (reassigning pointers whenever necessary) 
        3. Add the removed element to the output list (obviously reassigning the .next pointers whenever necessary) 

      2. Code example 
      ```
      def selection_sort (unsorted): 
        let sorted = None 
        while unsorted is not None: 
          let largest = find_largest(unsorted) 
          unsorted = remove(largest, unsorted) 
          sorted = cons(largest, sorted) 
        return sorted
      ```
        1. O(n) time complexity 
        2. Tselection_sort(n) = n * (Tis not + Tfind_largest(n) + Tremove(n) + Tcons) 
        3. = n * (c1 + Tfind_largest(n) + tremove(n)) <- consolidate Tis not and Tcons because they are constant time 
        4. = n * (c1 + O(n) + O(n))
        5. = n * (c1 + 2 * O(n))
        6. = n * (c1 + O(n)) 
        7. = n * (O(n)) 
        8. = O(n^2) 

    2. Merge Sort  
      1. Very sophisticated
      2. Also known as "divide and conquer" 
      3. Process
        1. Split the array into sub-lists (even indices and odd indices) 
          1. You can also split them with other invariants 
        2. Recursively sort each sub-list 
        3. Merge the two, picking the smallest element from the lists by comparing them and then appending that to a new 
        list 

      3. Example 

      {4, 2, 1, 3, 7, 0}
      {4, 1, 7} and {2, 3, 0} 
      {1, 4, 7} and {0, 2, 3} 
      {0} compare 1 vs 0 
      {0,1} compare 1 vs 2 
      {0,1,2} compare 2 vs 4 
      {0,1,2,3} compare 3 vs 4 
      {0,1,2,3,4} compare 4 vs none 
      {0,1,2,3,4,7} compare 7 vs none 

      4. Example 
      ```
      def merge_sort(lst): 
        if lst is None or lst.next is None: return lst 
        # Step 1: split into two sub-lists 
        let o = odds(lst) 
        let e = evens(lst) 
        # Step 2: recursively sort each sub-list 
        o = merge_sort(o) 
        e = merge_sort(e) 
        # Step 3: merge the two sub-lists 
        return merge(o, e) 
      ```
      5. Time Complexity 
        1. Tmerge_sort(1) = Tis + T.next + Tis 
        2. Tmerge_sort(n) = Tis + T.next + Tis + Todds(n) + Tmerge_sort(n/2) 
                          + Tevens(n) + Tmerge_sort(n/2) + Tmerge(n) 
        3. = c1 + Todds(n) + Tmerge(n/2) + Tevens(n) + Tmerge_sort(n/2) + Tmerge(n) 
        4. = c1 + O(n) <- we need to traverse the entire list 
        5. O(n) + 2 * Tmerge_sort(n/2) 
        6. Tmerge_sort(1) = c1 
        7. Tmerge_sort(n) = n + 2 * Tmerge_sort(n/2) 
        8. = n + 2 (n/2 + 2 * Tmerge_sort(n/4)) 
        9. = n + 2 * n/2 + 4 * n/4 + 8 * Tmerge_sort(n/8)) 
        10. n + 2 * n/2 + 4 * n/4 + ... + n * c1 
        11. n + n + n + n + n * c1 

        12. O (n*log(n)) time complexity 
          1. Between O(n) and O(n^2) 
      5. Time Complexity Chart 
      n, n^2, nlog(n) 
      16, 256, 64
      32, 1024, 160
      64, 4096, 384
      
      6. Dawson's First Law 
        1. O(n^2) is the "sweet spot" of badly scaling algorithms:
          1. Fast enough to make it into production, but slow enough to make things fall down once it gets there 

3. Return to Big-O Inequalities
  1. For constants j, k where j<k: 
    1 <<< logn <<< n^j <<< n^k <<< n^k logn <<< j^n <<< k^n <<< n^n
  2. Constants ar eless than logs, which are less than
  polynomials, which are less than higher-degree 
  polynomials, which are less than poly-log (for same k),
  which are less than exponentials , which are less than higher-base
  exponentials, which are less than this horror
  3. n^3 is intractable, j^n is a non-starter

4.
5.



